{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bYvI5U3_I3Cc"
   },
   "outputs": [],
   "source": [
    "!pip install keras_core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DDbYrCwXI2AA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import pandas as pd\n",
    "\n",
    "os.environ[\"KERAS_BACKEND\"] = \"torch\"\n",
    "import keras_core as keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gC8CYANLI2AC"
   },
   "source": [
    "# The Perceptron and beyond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j7ngQ26pI2AD"
   },
   "source": [
    "In this section, we consider a regression task where the underlying data generation process is as follows:\n",
    "$$ y = - \\sin{(0.8 * x)}$$\n",
    "\n",
    "Note that in this example there is no noise in the data, a case that you will rarely encounter in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tHrthUVqI2AD"
   },
   "outputs": [],
   "source": [
    "# Defining our dataset\n",
    "\n",
    "x_train = torch.linspace(0, 3, 50)\n",
    "y_train = - torch.sin(0.8*x_train)\n",
    "\n",
    "# Plotting the dataset\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.scatter(x_train, y_train, color= \"firebrick\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_s1JQhqlI2AD"
   },
   "source": [
    "The following function can be used to instantiate the perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TMc0-rWII2AE"
   },
   "outputs": [],
   "source": [
    "def perceptron(input_shape= [1], units= 1, activation= None):\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Input(shape= input_shape),\n",
    "            keras.layers.Dense(units= units, activation= activation)\n",
    "        ]\n",
    "    )\n",
    "    return model\n",
    "\n",
    "linear_model = perceptron()\n",
    "\n",
    "# You can vizualize a summary of your model by commenting out the following line.\n",
    "# Here it is an affine function in dimension 1, thus having two parameters.\n",
    "\n",
    "# linear_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pl-Q4dCqI2AE"
   },
   "source": [
    "To fit a model in keras, one must first compile it, that is specify the loss and optimizer. Here they are respectively chosen as the mean squared error and gradient descent. Observe that the batch size is the same as the size of the training dataset, resulting in batch gradient descent.\n",
    "\n",
    "You can play with the learning rate and number of epochs to understand how the training algorithm behaves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e4VoDYVRI2AE"
   },
   "outputs": [],
   "source": [
    "linear_model.compile(\n",
    "    loss = keras.losses.MeanSquaredError(),\n",
    "    optimizer = keras.optimizers.SGD(learning_rate= 0.2)\n",
    ")\n",
    "history = linear_model.fit(x_train, y_train, epochs= 35, batch_size= 50)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('training loss')\n",
    "plt.plot(history.history['loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNw53Cn6I2AF"
   },
   "source": [
    "Having trained the model, we can now use it in prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aRV-VK2cI2AF"
   },
   "outputs": [],
   "source": [
    "y_pred_linear = linear_model.predict(x_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.scatter(x_train, y_train, color='firebrick', label= 'true function')\n",
    "plt.plot(x_train, y_pred_linear, color= 'teal', label= 'linear model')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yRwRrlxI2AF"
   },
   "source": [
    "Beyond this simple perceptron, we now consider a 2-layers neural network model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-GfcxfP5I2AF"
   },
   "outputs": [],
   "source": [
    "def neural_net_2layers(input_shape= [1], units= 16, activation= 'sigmoid'):\n",
    "    model = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Input(shape= input_shape),\n",
    "            keras.layers.Dense(units= units, activation= activation),\n",
    "            keras.layers.Dense(units= 1)\n",
    "        ]\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_6oaU3keI2AF"
   },
   "source": [
    "You can train the model, and play with the hyperparameters to answer the questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SiZ3UilBI2AF"
   },
   "outputs": [],
   "source": [
    "net = neural_net_2layers(units= 10)\n",
    "\n",
    "net.compile(\n",
    "    loss = keras.losses.MeanSquaredError(),\n",
    "    optimizer = keras.optimizers.Adam(learning_rate= 0.1)\n",
    ")\n",
    "\n",
    "history = net.fit(x_train, y_train, epochs= 300, batch_size= 25, verbose= 0)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('training loss')\n",
    "plt.plot(np.log(history.history['loss']))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Spy3zF4FI2AF"
   },
   "outputs": [],
   "source": [
    "y_pred_2l = net.predict(x_train)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.scatter(x_train, y_train, color='firebrick', label = 'true function')\n",
    "plt.plot(x_train, y_pred_linear, color= 'teal', label = 'linear model')\n",
    "plt.plot(x_train, y_pred_2l, color= 'indigo', label = '2-l net')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YQeAW84_I2AF"
   },
   "source": [
    "# In the wild jungle of training algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SJqAbHq-I2AF"
   },
   "source": [
    "## Small model\n",
    "\n",
    "We first start by defining a simple dataset based on a modified cosine surface. This will allows us to test small models, and in particular 2nd order optimization algorithms. While <tt>keras</tt> is high-level deep learning library, we will dive into the depths of <tt>pytorch</tt> to have more control over the optimization schemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0Kp0jkcVI2AG"
   },
   "outputs": [],
   "source": [
    "print(os.getcwd())\n",
    "def f(x, y):\n",
    "    return(torch.sin(0.8*(x**2 + y**2)) / (x**2 + y**2)**(0.9))\n",
    "\n",
    "noise = 0\n",
    "mesh_size = 40\n",
    "t = torch.linspace(-3, 3, mesh_size)\n",
    "x, y = torch.meshgrid(t, t)\n",
    "z = f(x, y) + noise * torch.randn(mesh_size, mesh_size)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = plt.axes(projection='3d')\n",
    "\n",
    "ax.plot_surface(x, y, z,cmap='viridis', edgecolor='none')\n",
    "ax.set_title('Surface plot')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# dataset creation\n",
    "x_train = torch.vstack([x.ravel(), y.ravel()]).T\n",
    "y_train = z.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9G7LRc-I2AG"
   },
   "source": [
    "The following class encodes the architecture of a neural network with two hidden layers, for when the input data is 2-dimensional. We provide a simplified <tt>fit</tt> method to train the neural network. You are encouraged to understand the code, and what executing the functions will return so as to draw legitimate conclusions about the experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ocLzEFYZI2AG"
   },
   "outputs": [],
   "source": [
    "class Net3L(nn.Module):\n",
    "\n",
    "    def __init__(self, n_neurons_l1= 10, n_neurons_l2= 10):\n",
    "        super(Net3L, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(2, n_neurons_l1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_neurons_l1, n_neurons_l2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(n_neurons_l2, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear_relu_stack(x)\n",
    "\n",
    "    def fit(self, x_train, y_train, optimizer, batch_size, n_epochs_max):\n",
    "        # Instantiate the train loader\n",
    "        train_data = TensorDataset(x_train, y_train)\n",
    "        train_loader = DataLoader(dataset =train_data, batch_size= batch_size, shuffle= True)\n",
    "        # Beginning the optimization algorithm\n",
    "        loss_list = []\n",
    "        for n_epochs in range(n_epochs_max):\n",
    "            # We train for a fixed number of epochs\n",
    "            for x_batch, y_batch in train_loader:\n",
    "                def closure():\n",
    "                    optimizer.zero_grad()\n",
    "                    output = self.forward(x_batch)\n",
    "                    loss = nn.MSELoss()(output.squeeze(-1), y_batch)\n",
    "                    loss.backward()\n",
    "                    return loss\n",
    "                loss = optimizer.step(closure)\n",
    "            loss_list.append(loss.item())\n",
    "        return loss_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIXQRSdKI2AG"
   },
   "source": [
    "The cell below gives an example of what happens when the training is done using the L-BFGS algorithm. It can be adapted to other algorithms by modifying the arguments passed to the <tt>fit</tt> method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "66ZRcYr4I2AG"
   },
   "outputs": [],
   "source": [
    "net = Net3L(50, 50)\n",
    "loss_list = net.fit(x_train, y_train, torch.optim.LBFGS(net.parameters(), lr=1, max_iter=1, line_search_fn='strong_wolfe'), 1600, 2000)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('Epoch number')\n",
    "plt.ylabel('Training MSE')\n",
    "plt.plot(loss_list)\n",
    "plt.show()\n",
    "\n",
    "y_pred = net(x_train)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(10, 10), subplot_kw={'projection': '3d'})\n",
    "#\n",
    "ax[0].plot_trisurf(x_train[:, 0], x_train[:, 1], y_pred.detach().numpy().squeeze(-1), vmin=y_train.min(), cmap=cm.viridis)\n",
    "ax[0].set_xlabel('x')\n",
    "ax[0].set_ylabel('y')\n",
    "ax[0].set_zlabel('z')\n",
    "ax[0].set_title('Surface')\n",
    "#\n",
    "ax[1].plot_trisurf(x_train[:, 0], x_train[:, 1], (y_pred.detach().squeeze(-1)-y_train)**2, vmin=0, cmap=cm.viridis)\n",
    "ax[1].set_xlabel('x')\n",
    "ax[1].set_ylabel('y')\n",
    "ax[1].set_zlabel('z')\n",
    "ax[1].set_title('Squared residuals')\n",
    "plt.show()\n",
    "\n",
    "print('Training error:', (nn.MSELoss()(y_pred.squeeze(-1), y_train)).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ek1inGd3I2AG"
   },
   "outputs": [],
   "source": [
    "# Example with a list of optimizers\n",
    "net = Net3L(50, 50)\n",
    "# We create copies of this net so that every optimizer starts with the same initialization.\n",
    "net_list = [copy.deepcopy(net) for i in range(5)]\n",
    "# The number of epochs and other hyperparameters can (and should) be adapted to the point you're trying to make.\n",
    "n_epochs = 2500\n",
    "optim1 = torch.optim.SGD(params=net_list[0].parameters(), lr=0.05)\n",
    "optim2 = torch.optim.SGD(params=net_list[1].parameters(), lr=0.1)\n",
    "optim3 = torch.optim.SGD(params=net_list[2].parameters(), lr=0.1, momentum=0.9, nesterov= True)\n",
    "optim4 = torch.optim.Adam(params=net_list[3].parameters())\n",
    "optim5 = torch.optim.LBFGS(params=net_list[4].parameters(), lr=1, max_iter=1, line_search_fn='strong_wolfe')\n",
    "\n",
    "optimizer_list = [optim1, optim2, optim3, optim4, optim5]\n",
    "batch_sizes_list = [1600, 40, 400, 400, 1600]\n",
    "times_list = []\n",
    "loss_meta_list = []\n",
    "\n",
    "for i, optimizer in enumerate(optimizer_list):\n",
    "    t0 = time.time()\n",
    "    loss_list = net_list[i].fit(x_train, y_train, optimizer= optimizer, batch_size= batch_sizes_list[i], n_epochs_max= n_epochs)\n",
    "    t1 = time.time()\n",
    "    times_list.append(t1-t0)\n",
    "    loss_meta_list.append(loss_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OTmwEQ0AI2AG"
   },
   "outputs": [],
   "source": [
    "colors = cm.tab10(torch.linspace(0, 1, 5))\n",
    "plt.figure()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Training MSE')\n",
    "for i in range(5):\n",
    "    plt.plot(loss_meta_list[i], color= colors[i])\n",
    "plt.legend([f\"optim{i+1}: {round(times_list[i], 2)}s\" for i in range(5)])\n",
    "plt.show()\n",
    "print('Training times:', [f\"optim{i+1}: {times_list[i]}s\" for i in range(5)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZglIKosuI2AG"
   },
   "source": [
    "## Towards a bigger model\n",
    "\n",
    "We now switch to a bigger model, used to perform classification over the MNIST dataset. This example is widely known in the machine learning community, and we follow the code in the footsteps of François Chollet, the creator of the <tt>keras</tt> library (https://keras.io/examples/vision/mnist_convnet/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJz_yYUQI2AG"
   },
   "outputs": [],
   "source": [
    "# Model / data parameters\n",
    "num_classes = 10\n",
    "input_shape = (28, 28, 1)\n",
    "\n",
    "# Load the data and split it between train and test sets\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Scale images to the [0, 1] range\n",
    "x_train = x_train.astype(\"float32\") / 255\n",
    "x_test = x_test.astype(\"float32\") / 255\n",
    "# Make sure images have shape (28, 28, 1)\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "print(\"x_train shape:\", x_train.shape)\n",
    "print(x_train.shape[0], \"train samples\")\n",
    "print(x_test.shape[0], \"test samples\")\n",
    "\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dP5zNEHvI2AG"
   },
   "outputs": [],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        keras.layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        keras.layers.Flatten(),\n",
    "        keras.layers.Dropout(0.5),\n",
    "        keras.layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IqHQUICFI2AG"
   },
   "source": [
    "The following cell deals with the training of the model. Can you change the optimizer while still retaining the capabilities of the model trained with adam ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yjqjpCG5I2AH"
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 15\n",
    "\n",
    "# Modify the optimizer here\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_split=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "un0k-BScI2AH"
   },
   "outputs": [],
   "source": [
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print(\"Test loss:\", score[0])\n",
    "print(\"Test accuracy:\", score[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uev7szZMI2AH"
   },
   "source": [
    "# A personal regression example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bBEWHpLaI2AH"
   },
   "source": [
    "\n",
    "For this exercise, you will work on a dataset that is built from your personal student number as indicated in the assignment. If you have trouble with loading the data, place the data.csv file in your working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k5xddMmj3_6v"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  data = pd.read_csv(\"data.csv\", header= None).values.T\n",
    "  print(\"Succesfully loaded data\")\n",
    "except:\n",
    "  try:\n",
    "    data_url = \"https://github.com/KULasagna/ANN_DL_public/blob/master/session1/data.csv?raw=true\"\n",
    "    data = pd.read_csv(data_url, header= None).values.T\n",
    "    print(\"Succesfully loaded data\")\n",
    "  except:\n",
    "    print(f\"Please download the data.csv file from Toledo and place it in your working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3yMBRJFCI2AH"
   },
   "outputs": [],
   "source": [
    "def dataset_generation(d1, d2, d3, d4, d5):\n",
    "    T_new = (d1 * data[:, 2] + d2 * data[:, 3] + d3 * data[:, 4] + d4 * data[:, 5] + d5 * data[:, 6])/(d1 + d2 + d3 + d4 + d5)\n",
    "    return T_new\n",
    "\n",
    "x_values = data[:, 0:2]\n",
    "\n",
    "# Put your own (decreasingly ordered) student number in the function\n",
    "# e.g. u1024659 --> T_new =  dataset_generation(9, 6, 5, 4, 2)\n",
    "\n",
    "T_new =  dataset_generation(1, 1, 1, 1, 1)\n",
    "\n",
    "# Random subsampling to get the working dataset\n",
    "idx = np.random.permutation(13600)\n",
    "\n",
    "# training set\n",
    "x_train = x_values[idx[0:2000]]\n",
    "y_train = T_new[idx[0:2000]]\n",
    "\n",
    "# test set\n",
    "x_test = x_values[idx[2000:3000]]\n",
    "y_test = T_new[idx[2000:3000]]\n",
    "\n",
    "\n",
    "print('Number of training datapoints:', x_train.shape[0])\n",
    "print('Number of testing datapoints:', x_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rVI-DXCVI2AH"
   },
   "source": [
    "You can now plot the surface of the dataset. (Hint: use the function <tt> plot_trisurf </tt>)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y1mj1qmGI2AH"
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(subplot_kw={'projection': '3d'})\n",
    "# TODO\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yF-22R1I2AH"
   },
   "source": [
    "In the following, we give an example of how to train a model. When you train yours, do not forget to validate it ! Hint: it is not enough to look at the training error. The hyperparameter values have voluntarily be set to give suboptimal results, you can do better than the proposed architecture !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SdT1GOXrI2AH"
   },
   "outputs": [],
   "source": [
    "net = keras.Sequential(\n",
    "        [\n",
    "            keras.layers.Input(shape= [x_train.shape[1]]),\n",
    "            keras.layers.Dense(units= 3, activation='tanh'),\n",
    "            keras.layers.Dense(units= 1)\n",
    "        ]\n",
    "    )\n",
    "\n",
    "net.compile(\n",
    "    loss = keras.losses.MeanSquaredError(),\n",
    "    optimizer = keras.optimizers.Adam(learning_rate= 0.05)\n",
    ")\n",
    "\n",
    "callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "history = net.fit(x_train, y_train, validation_split= 0.2, epochs= 20, batch_size= 64, callbacks= [callback], verbose= 0)\n",
    "\n",
    "plt.figure()\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('training loss')\n",
    "plt.plot(history.history['loss'], color= \"firebrick\")\n",
    "plt.plot(history.history['val_loss'], color= 'teal')\n",
    "plt.show()\n",
    "print('Last training error:', history.history['loss'][-1])\n",
    "print('Last validation error:', history.history['val_loss'][-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eSSO-vvKI2AH"
   },
   "source": [
    "Once a model is trained, you can use it to assess the final performance of the model on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6GeDr3sDI2AH"
   },
   "outputs": [],
   "source": [
    "y_pred = net.predict(x_test)\n",
    "print('Final test error:', 0.5*((y_pred - y_test)**2).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mY3VV5OAI2AL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.8 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
