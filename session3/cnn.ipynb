{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Artificial Neural Networks and Deep Learning  \n",
        "\n",
        "\n",
        "\n",
        "##Assignment 3.2 - Convolutional Neural Networks\n",
        "\n",
        "Prof. Dr. Ir. Johan A. K. Suykens     \n",
        "\n",
        "In this file, we will implement a convolutional neural network from scratch.\n",
        "\n",
        "We only consider classification tasks here in the session.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XEDrFr3_VWQm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k06FYdbfVReE"
      },
      "outputs": [],
      "source": [
        "# Please first load your google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Please go to Edit > Notebook settings > Hardware accelerator > choose \"T4 GPU\"\n",
        "# Now check if you have loaded the GPU successfully\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "tBcSkiFpVmTq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Convolutional Neural Networks\n",
        "In this section, we implement CNN from scrath and train it on the MNIST dataset.\n",
        "\n",
        "We first load the dataset as follows:"
      ],
      "metadata": {
        "id": "C2KVP3ThVt4T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn, optim, functional, utils\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import torchvision\n",
        "from torchvision import datasets, utils\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "import time, os\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "# use seaborn\n",
        "import seaborn as sns\n",
        "\n",
        "# Apply the default theme\n",
        "sns.set_theme()\n",
        "plt.rcParams[\"figure.dpi\"] = 100\n",
        "plt.rcParams['savefig.dpi'] = 300\n",
        "\n",
        "\n",
        "def get_mnist_loader(batch_size=100, num_train_samples = 10000, num_test_samples = 2000):\n",
        "    \"\"\"\n",
        "\n",
        "    :return: train_loader, test_loader\n",
        "    \"\"\"\n",
        "    train_dataset = MNIST(root='../data',\n",
        "                          train=True,\n",
        "                          transform=torchvision.transforms.ToTensor(),\n",
        "                          download=True)\n",
        "    test_dataset = MNIST(root='../data',\n",
        "                         train=False,\n",
        "                         transform=torchvision.transforms.ToTensor(),\n",
        "                         download=True)\n",
        "\n",
        "    # Randomly select a subset of samples\n",
        "    train_indices = torch.randperm(len(train_dataset))[:num_train_samples]\n",
        "    test_indices = torch.randperm(len(test_dataset))[:num_test_samples]\n",
        "\n",
        "    # Create subset samplers to be used in the dataloader\n",
        "    train_subset_sampler = torch.utils.data.SubsetRandomSampler(train_indices)\n",
        "    test_subset_sampler = torch.utils.data.SubsetRandomSampler(test_indices)\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                               batch_size=batch_size,\n",
        "                                               sampler = train_subset_sampler)\n",
        "    test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                              batch_size=batch_size,\n",
        "                                              sampler = test_subset_sampler)\n",
        "\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "-X7wZ07qVny5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build CNN from scratch\n",
        "Our CNN is of 4 convolutional layers and 2 fully connected layers.\n",
        "You can try a different amount of layers by simply add more ``self.conv`` layers inside  ``__init__`` and ``forward`` in the following code. Please be careful with the dimensions of the convolutional layers when changing parameters such as convolution channels, kernel_size and stride."
      ],
      "metadata": {
        "id": "l8DOJpeycH1L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CNNnet(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNNnet,self).__init__()\n",
        "        self.conv1 = torch.nn.Sequential(\n",
        "            # 2-D convolution\n",
        "            torch.nn.Conv2d(in_channels=1, # input channels\n",
        "                            out_channels=16, # convolution channels\n",
        "                            kernel_size=3, # kernel size of the convolution\n",
        "                            stride=2, # stride of the kernel\n",
        "                            padding=1), # padding with 0\n",
        "            # Data is normalized before entering ReLUï¼Œnum_features=batch_size*num_features*height*width\n",
        "            # Output is with size (N,C,W,H), i.e., (batch_size, channels, width, height)\n",
        "            torch.nn.BatchNorm2d(16), # the input dimensionality of BatchNorm2d should match the number of convolution channels\n",
        "            # Set activation function\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "        self.conv2 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(16,32,3,2,1), # be sure to have the number of input channels match the output channels of the previous layer\n",
        "            torch.nn.BatchNorm2d(32),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "        self.conv3 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(32,32,3,2,1),\n",
        "            torch.nn.BatchNorm2d(32),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "        self.conv4 = torch.nn.Sequential(\n",
        "            torch.nn.Conv2d(32,64,2,2,0),\n",
        "            torch.nn.BatchNorm2d(64),\n",
        "            torch.nn.ReLU()\n",
        "        )\n",
        "        # set fully connected layer\n",
        "        self.mlp1 = torch.nn.Linear(2*2*64,100) # input dimension should match the output dimension from the previous layer\n",
        "        # final output is of dimension 10 since\n",
        "        # MNIST is a 10-class classification dataset\n",
        "        self.mlp2 = torch.nn.Linear(100,10)\n",
        "\n",
        "    # feedforward\n",
        "    def forward(self,x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.conv3(x)\n",
        "        x = self.conv4(x)\n",
        "        # flatten tensor to 1-D\n",
        "        x = self.mlp1(x.view(x.size(0),-1))\n",
        "        x = self.mlp2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vwTxoe2VV7jX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training function\n",
        "The training process includes loading the model, setting the optimizer, and running on the training set for epochs.\n",
        "After each training epoch, we evaluate the model on the test set to record the test accuracy."
      ],
      "metadata": {
        "id": "bsGqOf9mk6yR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, train_epoch, model_save, train_loader, test_loader) :\n",
        "    # Push model on to GPU if available\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    loss_func = nn.CrossEntropyLoss( )\n",
        "    # Use Adam optimizer\n",
        "    optimizer = torch.optim.Adam(model.parameters( ),lr=0.001)\n",
        "\n",
        "    acc_count = []  # record the test accuracy\n",
        "    for epoch in range(train_epoch):\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for i, (x, y) in enumerate(train_loader):\n",
        "            x = x.to(device)  # torch,Size([128,1,28,28])\n",
        "            y = y.to(device)   # torch.Size([128])\n",
        "            # Output of the model\n",
        "            out = model(x)  # torch.Size([128,10])\n",
        "            # Compute loss\n",
        "            loss = loss_func(out, y)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()  # backpropagation\n",
        "            optimizer.step()  # update the network parameters\n",
        "\n",
        "            # save the model checkpoint every 20 iterations\n",
        "            if i % 20 == 0:\n",
        "                print('Training Loss:{:.6f} Batch {}/{} '.format(loss.item(), i, len(train_loader)))\n",
        "                torch.save(model, model_save)\n",
        "\n",
        "        # evaluate the model after each training epoch\n",
        "        with torch.no_grad():\n",
        "            model.eval()\n",
        "\n",
        "            true_pred = torch.zeros(1).to(device)\n",
        "            nb_sample = 0\n",
        "\n",
        "            for inputs, targets in test_loader:\n",
        "                inputs = inputs.to(device)\n",
        "                targets = targets.to(device)\n",
        "                outputs = model(inputs)\n",
        "\n",
        "                _, pred = torch.max(outputs, dim=1)\n",
        "\n",
        "                true_pred = true_pred + torch.sum(pred == targets).type(torch.FloatTensor)\n",
        "                nb_sample += len(inputs)\n",
        "\n",
        "            acc = true_pred / nb_sample\n",
        "            acc = acc.item()\n",
        "\n",
        "            acc_count.append(acc)\n",
        "\n",
        "            print('Epoch {:d}, Test Accuracy {:.3f} %'.format(epoch, acc * 100))\n",
        "\n",
        "    return acc_count"
      ],
      "metadata": {
        "id": "tHe1PyUucZGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Let's start training!\n",
        "The network is trained for 20 epochs, batch size of 100.\n",
        "\n",
        "You can adjust the hyperparameters here if necessary."
      ],
      "metadata": {
        "id": "fqXo_JIWla_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# You can adjust the hyperparameters here if needed\n",
        "train_epoch = 20\n",
        "batch_size = 100\n",
        "shuffle = True\n",
        "\n",
        "# Define model\n",
        "model = CNNnet()\n",
        "\n",
        "# where to save the model\n",
        "model_save = './MNIST_CNN/ckpt.pth'\n",
        "\n",
        "import os\n",
        "if not os.path.exists('MNIST_CNN'):\n",
        "    os.mkdir('MNIST_CNN')"
      ],
      "metadata": {
        "id": "cMjq_kz8guO_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# load datasets\n",
        "train_loader, test_loader = get_mnist_loader(batch_size=batch_size)\n",
        "\n",
        "# train the model\n",
        "acc_count = train(model, train_epoch=train_epoch, model_save=model_save, train_loader=train_loader, test_loader=test_loader)"
      ],
      "metadata": {
        "id": "yhijmumReHXc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the accuracy curves w.r.t. epochs\n",
        "plt.plot(acc_count, marker='.')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.title('Test Accuracy of CNN')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "V4gtbMW6jBRC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}