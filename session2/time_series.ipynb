{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wVhPZfkbyPoW"
   },
   "source": [
    "# Time-series Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "51ihHWF2ns01"
   },
   "source": [
    "A time series is a sequence of observations, ordered in time. Forecasting involves training a model on historical data and using them to predict future observations. A simple example is a linear auto-regressive model. The linear auto-regressive (AR) model of a time-series $Z_t$ with $t=1,2,\\dots,\\infty$ is given by\n",
    "\n",
    "$$\\hat{z}_t = a_1 z_{t-1} + a_2 z_{t-2} + \\cdots + a_p z_{t-p},$$\n",
    "\n",
    "with $a_i \\in \\mathbb{R}$ for $i=1, \\dots, p$ and $p$ the model lag. The prediction for a certain time $t$ is equal to a weighted sum of the previous values up to a certain lag $p$. In a similar way, the nonlinear (NAR) variant is described as\n",
    "\n",
    "$$\\hat{z}_t = f(z_{t-1}, z_{t-2}, \\dots, z_{t-p}).$$\n",
    "\n",
    "The figure below visualizes this process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xlur6AxPpiuF"
   },
   "source": [
    "<img src=\"https://raw.githubusercontent.com/KULasagna/ANN_DL_public/master/assets/nar.jpg\" width=\"600\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drsFGMT3nvb9"
   },
   "source": [
    "Remark that in this way, the time-series identification can be written as a classical black-box regression modeling problem $\\hat{y}_t=f(x_t)$ with $y_t=z_t$ and $x_t=[z_{t-1}, z_{t-2}, \\dots, z_{t-p}]$. When preparing the dataset and applying train/validation/test splits, it is important to prevent *data leakage* by respecting the temporal information flow. More precisely, a datapoint $z_t$ should not be part of two splits &mdash; either as input $x_t$ or target $y_t$ &mdash; and training (or validation) sets should not contain datapoints that occur after test datapoints.\n",
    "\n",
    "In this notebook, we work on the time-series prediction problem using a multilayer perceptron (MLP) and a long short-term memory network (LSTM)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wq7pzE-qs2cm"
   },
   "source": [
    "## Colab Setup\n",
    "This part is only required when running this notebook \"in the cloud\" on [Google Colab](https://colab.research.google.com). When running it locally, skip this part and go to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jLmJE3TEs8-U"
   },
   "outputs": [],
   "source": [
    "# Load some auxiliary files from github.\n",
    "!wget https://raw.githubusercontent.com/KULasagna/ANN_DL_public/master/session2/SantaFe.npz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y5kT1d__qweW"
   },
   "source": [
    "## Setup\n",
    "Import all the necessary modules used throughout this notebook and define some helper methods to work with timeseries data and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zFNzy5EJqy0d"
   },
   "outputs": [],
   "source": [
    "# Import the required modules for this notebook\n",
    "from dataclasses import dataclass\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy\n",
    "from sklearn.model_selection import TimeSeriesSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCPfrPvhhwLQ"
   },
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Fold:\n",
    "  train_idxs: np.ndarray\n",
    "  val_idxs: np.ndarray\n",
    "\n",
    "def prepare_timeseries(timeseries, lag, validation_size=0, validation_folds=0):\n",
    "  # Generate train (and validation) sets for the given timeseries and lag\n",
    "  data = scipy.linalg.hankel(timeseries[:-lag], timeseries[-lag-1:-1])\n",
    "  targets = timeseries[lag:]\n",
    "  if validation_size > 0 and validation_folds > 0:\n",
    "    tss = TimeSeriesSplit(test_size=validation_size, gap=lag)\n",
    "    tss.n_splits = validation_folds\n",
    "    folds = [Fold(train_idxs, val_idxs) for (train_idxs, val_idxs) in tss.split(data)]\n",
    "    return data, targets, folds\n",
    "  return data, targets\n",
    "\n",
    "def shift(window, values):\n",
    "  # Append new values to the given window (dropping the oldest values)\n",
    "  result = np.empty(window.shape)\n",
    "  values = np.atleast_1d(values)\n",
    "  s = values.shape[0]\n",
    "  result[:-s] = window[s:]\n",
    "  result[-s:] = values\n",
    "  return result\n",
    "\n",
    "def normalize(timeseries, params=None):\n",
    "  # Apply z-score normalization to the given timeseries\n",
    "  if params is None:\n",
    "    params = (np.mean(timeseries), np.std(timeseries))\n",
    "  mu, sigma = params\n",
    "  normalized = (timeseries - mu) / sigma\n",
    "  return normalized, params\n",
    "\n",
    "def rescale(timeseries, params):\n",
    "  # Rescale the normalized timeseries back to its original values\n",
    "  mu, sigma = params\n",
    "  rescaled = mu + timeseries * sigma\n",
    "  return rescaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aNiSpu-WiwIV"
   },
   "outputs": [],
   "source": [
    "def plot_history(history, title, filename=None):\n",
    "  # Plot the train and validation loss curves\n",
    "  fig, ax = plt.subplots(figsize=(10, 3))\n",
    "  ax.semilogy(history.history['loss'], label='Train')\n",
    "  if 'val_loss' in history.history:\n",
    "    ax.semilogy(history.history['val_loss'], label='Validation')\n",
    "  ax.legend()\n",
    "  ax.set_xlabel('Epochs')\n",
    "  ax.set_ylabel('Loss')\n",
    "  plt.title(title)\n",
    "  if filename is not None:\n",
    "    plt.savefig(f\"{filename}.svg\")\n",
    "  plt.show()\n",
    "\n",
    "def plot_timeseries(timeseries_dict, title, filename=None):\n",
    "  # Plot the given timeseries\n",
    "  fig, ax = plt.subplots(figsize=(10, 3))\n",
    "  for label, (start, ts) in timeseries_dict.items():\n",
    "    ax.plot(start + np.arange(len(ts)), ts, label=label)\n",
    "  ax.legend()\n",
    "  ax.set_xlabel('Timestep')\n",
    "  ax.set_ylabel('Laser intensity')\n",
    "  plt.title(title)\n",
    "  if filename is not None:\n",
    "    plt.savefig(f\"{filename}.svg\")\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3qGqFrCZpbfd"
   },
   "source": [
    "## Santa Fe Laser Dataset\n",
    "The Santa Fe laser dataset is obtained from a chaotic laser which can be described as a nonlinear dynamical system. The first $1000$ data points can be used for training and validation purposes. The aim is to predict the next $100$ points (it is forbidden to include these points in the training or validation sets!). Both datasets are stored in the `SantaFe.npz` file and are visualized below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "6pTA6Tb2tVCv",
    "outputId": "8af2ad8f-9f4d-4f68-d007-d07149440f33"
   },
   "outputs": [],
   "source": [
    "santafe = np.load('SantaFe.npz')\n",
    "train_series = santafe['A']\n",
    "test_series = santafe['Acont']\n",
    "plot_timeseries({\n",
    "    'Train': (0, train_series),\n",
    "    'Test': (len(train_series), test_series)\n",
    "}, 'Santa Fe laser dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MEPf2pX1ua62"
   },
   "source": [
    "To train the various nonlinear autoregressive models, it will be useful to prepare the timeseries dataset beforehand. We start by normalizing the train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "551MMyXmRXBf"
   },
   "outputs": [],
   "source": [
    "normalized, params = normalize(santafe['A'])\n",
    "normalized_test, _ = normalize(santafe['Acont'], params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32tsE_fWcJHH"
   },
   "source": [
    "Next, the `prepare_timeseries` function is used to convert the timeseries into training data ($x_t$) and targets ($y_t$). Make sure you understand what the function does by trying it out on a small toy example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ziSIfh35sxYC",
    "outputId": "a4705f6b-5c0e-4f8f-815c-78daeb22ad3f"
   },
   "outputs": [],
   "source": [
    "timeseries = np.arange(10)  # Increase this number to create a larger time series\n",
    "lag = 3  # Try different values for the lag\n",
    "data, targets, folds = prepare_timeseries(timeseries, lag, validation_size=1, validation_folds=2)  # Examine the effect of the validation size and number of validation folds\n",
    "print(f\"Original timeseries: {timeseries}\")\n",
    "print(f\"Data for lag {lag}: {data}\")\n",
    "print(f\"Targets: {targets}\")\n",
    "for i, fold in enumerate(folds):\n",
    "  print(f\"Fold {i}:\")\n",
    "  print(f\"  Train:     data={data[fold.train_idxs]}, targets={targets[fold.train_idxs]}\")\n",
    "  print(f\"  Validate:  data={data[fold.val_idxs]}, targets={targets[fold.val_idxs]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzSE4nW_dVGQ"
   },
   "source": [
    "Once you understand the dataset structure, apply it to the normalized Santa Fe timeseries. You can come back to these cells later to change the values of the lag or validation parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzkVFMz2dmdz"
   },
   "outputs": [],
   "source": [
    "# @title Parameters { run: \"auto\" }\n",
    "lag = 10 # @param {type:\"slider\", min:1, max:100, step:1}\n",
    "validation_size = 100 # @param {type:\"slider\", min:10, max:200, step:1}\n",
    "validation_folds = 4 # @param {type:\"slider\", min:1, max:10, step:1}\n",
    "# This is the hidden dimension of the MLP and LSTM networks trained later in this notebook\n",
    "H = 20 # @param {type:\"slider\", min:10, max:100, step:1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "YiRjApLy4HYg",
    "outputId": "db1a8295-64e2-4ddb-dafa-cdae192c9ad4"
   },
   "outputs": [],
   "source": [
    "data, targets, folds = prepare_timeseries(normalized, lag, validation_size, validation_folds)\n",
    "# Plot train and validation data for each fold:\n",
    "for f, fold in enumerate(folds):\n",
    "  train_series = np.concatenate([data[fold.train_idxs[0]], targets[fold.train_idxs]])\n",
    "  val_series = np.concatenate([data[fold.val_idxs[0]], targets[fold.val_idxs]])\n",
    "  plot_timeseries({\"Train\": (0, train_series), \"Validation\": (len(train_series), val_series)}, f\"Fold {f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aYqrMMAjtQFv"
   },
   "source": [
    "## MLP\n",
    "We now train a first nonlinear autoregressive (NAR) model on the training set and choose the hyperparameters based on the predictive performance on the validation sets. This first model is a multilayer perceptron (MLP) and training is done in feedforward mode using the prepared training set\n",
    "\n",
    "$$\\hat{z}_t = w^\\top \\tanh(V[z_{t-1}; z_{t-2}; \\dots; z_{t-p}] + \\beta).$$\n",
    "\n",
    "In order to make predictions, the trained network is used in an iterative way as a recurrent network\n",
    "\n",
    "$$\\hat{z}_t = w^\\top \\tanh(V[\\hat{z}_{t-1}; \\hat{z}_{t-2}; \\dots; \\hat{z}_{t-p}] + \\beta).$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fx-b4pKJ17iy"
   },
   "outputs": [],
   "source": [
    "# Define the MLP network architecture\n",
    "def MLP(input_dim, hidden_dim, output_dim, activation='tanh'):\n",
    "  return keras.Sequential([\n",
    "    keras.layers.Input(shape=[input_dim]),  # Expect input of shape (B, I) with B batch size, I input size\n",
    "    keras.layers.Dense(units=hidden_dim, activation=activation),  # Output of shape (B, H) with H hidden feature size\n",
    "    keras.layers.Dense(units=output_dim)  # Output of shape (B, O) with O output size\n",
    "  ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4ftoGLnzbNX"
   },
   "source": [
    "Train the MLP on the last training fold. You can later put this code in a `for` loop to train on each of the training folds for determining the optimal hyperparameters using $N$-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u55ZltdH-LgP"
   },
   "outputs": [],
   "source": [
    "net = MLP(lag, H, 1)\n",
    "net.compile(\n",
    "  loss=keras.losses.MeanSquaredError(),\n",
    "  optimizer=keras.optimizers.Adam(learning_rate=0.01)\n",
    ")\n",
    "fold = folds[-1]\n",
    "history = net.fit(data[fold.train_idxs], targets[fold.train_idxs],\n",
    "                  validation_data=(data[fold.val_idxs], targets[fold.val_idxs]),\n",
    "                  epochs=200, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XdqT_7mpz82I"
   },
   "source": [
    "Plot the train and validation loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "qFTkxh0dzRoL",
    "outputId": "ba9032fa-ab2b-41f3-fac1-69f8003dab6d"
   },
   "outputs": [],
   "source": [
    "plot_history(history, \"MLP training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vtbbqdB73CLO"
   },
   "source": [
    "### **Exercise 1**\n",
    "Investigate the model performance with different lags and number of neurons. Discuss how the model looks and explain clearly how you tune the parameters and what the influence on the final prediction is. Which combination of parameters gives the best performance (MSE) on the test set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "3vf8lWCXCTlK",
    "outputId": "4eb8e43c-6022-4595-f660-7949e95c8e15"
   },
   "outputs": [],
   "source": [
    "# Evaluation on test set\n",
    "test_data = shift(data[-1], targets[-1])\n",
    "predictions = np.empty(normalized_test.shape)\n",
    "for t in range(len(predictions)):\n",
    "  predictions[t] = net.predict(test_data.reshape((1, lag)), verbose=0).squeeze()\n",
    "  test_data = shift(test_data, predictions[t])\n",
    "\n",
    "# Rescale predictions\n",
    "predictions_mlp = rescale(predictions, params)\n",
    "\n",
    "# Compute the mean squared error between the predictions and test set\n",
    "mse = np.mean((test_series - predictions_mlp)**2)\n",
    "print(\"The MSE on the test set is: {:.3f}\".format(mse))\n",
    "\n",
    "# Plot\n",
    "plot_timeseries({\"Test\": (1000, test_series), \"Predictions\": (1000, predictions_mlp)}, \"MLP prediction results on continuation of Santa Fe laser dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uLskk7m_osSv"
   },
   "source": [
    "## LSTM\n",
    "We now train the second model, which is a Long Short Term Memory (LSTM) network. These are a special kind of RNN, capable of learning long-term dependencies. LSTMs contain information outside the normal flow of the recurrent network in a gated cell. Information can be stored in, written to, or read from a cell, much like data in a computer's memory. The cell makes decisions about what to store and when to allow reads, writes and erasures, via gates that open and close. Those gates act on the signals they receive, and similar to the neural network's nodes, they block or pass on information based on its strength and importance, which they filter with their own sets of weights. Those weights, like the weights that modulate input and hidden states, are adjusted via the recurrent network's learning process. That is, the cells learn when to allow data to enter, leave or be deleted through the iterative process of making guesses, backpropagating the error, and adjusting weights via gradient descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q2DT-JT3ypAG"
   },
   "outputs": [],
   "source": [
    "# Define the LSTM network structure\n",
    "def LSTM(input_dim, hidden_dim, output_dim):\n",
    "  return keras.Sequential([\n",
    "    keras.layers.Input(shape=[None, input_dim], batch_size=1),  # LSTM layer expects input of shape (B, T, F) with B batch size, T timesteps, F feature size\n",
    "    keras.layers.LSTM(units=hidden_dim, return_sequences=True, stateful=True),  # Output of shape (B, T, H) with H hidden state size\n",
    "    keras.layers.Dense(units=output_dim)  # Output of shape (B, T, O) with O output size\n",
    "  ])\n",
    "\n",
    "class LSTMCallback(keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    self.model.reset_states()  # Make sure the LSTM's hidden state is reset after every epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zRK4-A3n7y-r"
   },
   "source": [
    "Train the LSTM on the last training fold. You can later put this code in a `for` loop to train on each of the training folds for determining the optimal hyperparameters using $N$-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6d6aj03us0xK"
   },
   "outputs": [],
   "source": [
    "net = LSTM(lag, H, 1)\n",
    "net.compile(\n",
    "  loss=keras.losses.MeanSquaredError(),\n",
    "  optimizer=keras.optimizers.Adam(learning_rate=0.001)\n",
    ")\n",
    "fold = folds[-1]\n",
    "history = net.fit(\n",
    "    data[fold.train_idxs].reshape((1, -1, lag)), targets[fold.train_idxs].reshape((1, -1)),\n",
    "    validation_data=(data[fold.val_idxs].reshape((1, -1, lag)), targets[fold.val_idxs].reshape((1, -1))),\n",
    "    epochs=150, callbacks=[LSTMCallback()], verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Oz7_At__Ax3"
   },
   "source": [
    "Plot the train and validation loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "UlzuuU4MXqTs",
    "outputId": "dc421c09-0ed2-4b58-fb68-8ca3469866c6"
   },
   "outputs": [],
   "source": [
    "plot_history(history, \"LSTM training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpZ1p-B4_vPz"
   },
   "source": [
    "### **Exercise 2**\n",
    "Investigate the model performance with different lags and number of hidden states. Discuss how the model looks and explain clearly how you tune the parameters and what the influence on the final prediction is. Which combination of parameters gives the best performance (MSE) on the test set?\n",
    "\n",
    "Compare the results of the recurrent MLP with the LSTM. Which model do you prefer and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 350
    },
    "id": "Dduj4Rbi6tza",
    "outputId": "4762080d-0fc7-4dda-95c9-0d20070b9e66"
   },
   "outputs": [],
   "source": [
    "# LSTM Evaluation\n",
    "net.reset_states()\n",
    "net.predict(data.reshape(1, -1, lag), verbose=0)\n",
    "test_data = shift(data[-1], targets[-1])\n",
    "predictions = np.empty(normalized_test.shape)\n",
    "for t in range(len(predictions)):\n",
    "  predictions[t] = net.predict(test_data.reshape((1, 1, lag)), verbose=0).squeeze()\n",
    "  test_data = shift(test_data, predictions[t])\n",
    "\n",
    "# Rescale predictions\n",
    "predictions_lstm = rescale(predictions, params)\n",
    "\n",
    "# Compute the mean squared error between the predictions and test set\n",
    "mse = np.mean((test_series - predictions_lstm)**2)\n",
    "print(\"The MSE on the test set is: {:.3f}\".format(mse))\n",
    "\n",
    "# Plot\n",
    "plot_timeseries({\"Test\": (1000, test_series), \"Predictions\": (1000, predictions_lstm)}, \"LSTM prediction results on continuation of Santa Fe laser dataset\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 333
    },
    "id": "wf-BQY0UBK6A",
    "outputId": "7e3eb8cc-01aa-41c3-95e5-f441400de9cf"
   },
   "outputs": [],
   "source": [
    "# Comparison of both models\n",
    "plot_timeseries({\"Test\": (1000, test_series), \"MLP\": (1000, predictions_mlp), \"LSTM\": (1000, predictions_lstm)}, \"Prediction results on continuation of Santa Fe laser dataset\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
